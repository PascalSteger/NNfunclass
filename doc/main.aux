\relax 
\bibstyle{mn2e}
\citation{Quarteroni2002}
\citation{Laemmel2004}
\citation{Rojas1996}
\citation{Jones1990}
\citation{Poggio1990}
\citation{Hopfield1982}
\citation{Stoop2003}
\newlabel{firstpage}{{}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:Introduction}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Regression Methods from Artificial Intelligence}{1}}
\citation{Stoop2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Aim}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}}
\newlabel{sec:Methods}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Input Data}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Function Set}{2}}
\newlabel{eq:class}{{1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Sampling of Input}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Sample input before and after normalization.}}{2}}
\newlabel{tab:ti1}{{1}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Normalization of Input}{2}}
\newlabel{eq:translating}{{7}{2}}
\newlabel{eq:scaling}{{9}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Network}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Basic Structure}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Function templates used for supervised learning. $f_{1n},f_{2n},f_{3n}$ are shown in the upper row, $f_{4n},f_{5n},f_{6n}$ in the lower one. Normalization of logarithmic plots reduces them to the same shape.}}{3}}
\newlabel{fig:classfct}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Basic topology of a forward connected network with two hidden layers. The shown configuration was used to map random input with 4 points at fixed $x$-coordinates to type and degree of the function, with the number of hidden neurons chosen such that the number of configurations to learn (4*4+1) is twice the number of neurons. Ability to classify noisy data is preserved.}}{3}}
\newlabel{fig:network_topology}{{2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Parameters}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Output}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Representation in Network}{3}}
\citation{Stoop2010}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sample visualization for fitting of an $x^2$}}{4}}
\newlabel{fig:vis}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Error}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Fitting, Backtransformation}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Visualization}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Implementation}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Computation environment}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Basic architecture of {\rm  Java} program}}{4}}
\newlabel{fig:flow}{{4}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Program Architecture}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}}
\newlabel{sec:Results}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Logic Functions}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Network applied on Training Set}{4}}
\newlabel{tab:logic}{{3.1}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Output of neural network of size $(n_{\rm  in},n_{\rm  hidden,1},n_{\rm  hidden,2},n_{\rm  out})=(2,8,8,1)$ on all logic functions $f(a,b)$ after 1000 learning steps with $\eta =1.0$. Error is taken to be square of norm, $e=\DOTSB \sum@ \slimits@ _{i=1}^N(o-t)^2$.}}{5}}
\newlabel{tab:logical}{{2}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Output of neural network applied on training data set.}}{5}}
\newlabel{tab:td1}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Parameter Dependence}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Noisy Input}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Learning Rate and Number of Iterations}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Shape of Sigmoid Function}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Classification error $e_1$ for each function as a function of noise is shown in black, where the label for the abscisse is $10(1-\delta )$ . Every panel holds all $f_{TD}$ for fix $T$. The red circles indicate $e2$ made by taking the rounded values for $T$ and $D$, averaged over all functions of a given type.}}{6}}
\newlabel{fig:e12a}{{5}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Classification error $e_1$ for each function as a function of learning steps is shown in black. Every panel holds all $f_{TD}$ for fix $T$. The values on the abscissa are scaled in 1000-steps. The red circles indicate $e2$ made by taking the rounded values for $T$ and $D$, averaged over all functions of a given type.}}{6}}
\newlabel{fig:e12nit}{{6}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Number of Hidden Neurons}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Limitations}{6}}
\bibdata{main}
\bibcite{Hopfield1982}{{1}{1982}{{Hopfield}}{{Hopfield}}}
\bibcite{Jones1990}{{2}{1990}{{Jones et~al.}}{{Jones, Lee, Barnes, Flake, Lee, Lewis \& Qian}}}
\bibcite{Poggio1990}{{3}{1990}{{Poggio \& Girosi}}{{Poggio \& Girosi}}}
\bibcite{Quarteroni2002}{{4}{2002}{{Quarteroni et~al.}}{{Quarteroni, Sacco \& Saleri}}}
\bibcite{Rojas1996}{{5}{1996}{{Rojas}}{{Rojas}}}
\bibcite{Stoop2010}{{6}{2010}{{Stoop}}{{Stoop}}}
\bibcite{Stoop2003}{{7}{2003}{{Stoop et~al.}}{{Stoop, Buchli, Keller \& Steeb}}}
\bibcite{Laemmel2004}{{8}{2004}{{U.~L\"ammel}}{{U.~L\"ammel}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Overall classification error as function of learning rate $\eta \in [0.1,1.0]$ and number of iterations $N_{{\rm  it}}=n_{{\rm  it}}/10^4\in [1,10]$.}}{7}}
\newlabel{tab:qof}{{4}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Total classification error $e$ as a function of sigmoid parameter $a$, with fixed $\eta =0.9$ and $n_{it}=10^6$.}}{7}}
\newlabel{fig:ea}{{7}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Dependence of quality of fit after $10^5$ iterations on number of hidden neurons, together with runtime characteristics.}}{7}}
\newlabel{tab:run}{{5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{7}}
\newlabel{sec:Discussion}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Summary}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Enhancements}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Acknowledgments}{7}}
\newlabel{sec:Acknowledgments}{{5}{7}}
\newlabel{lastpage}{{5}{7}}
